{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ujjawal/Downloads/creditcardfraud/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_fraud = df[df.Class == 0]\n",
    "fraud = df[df.Class == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284315, 31)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 31)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_fraud = not_fraud.sample(fraud.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 31)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([fraud,not_fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6108</td>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6329</td>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 31)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop('index' , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "1   472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "2  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "3  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "4  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0 -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211  0.320198   \n",
       "1  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966 -0.293803   \n",
       "2  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726 -0.087330   \n",
       "3 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207 -0.053502   \n",
       "4  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805 -1.632653   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "1  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "2 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "3  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "4  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984, 31)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      7526.000000\n",
       "V1           0.008430\n",
       "V2           4.137837\n",
       "V3          -6.240697\n",
       "V4           6.675732\n",
       "V5           0.768307\n",
       "V6          -3.353060\n",
       "V7          -1.631735\n",
       "V8           0.154612\n",
       "V9          -2.795892\n",
       "V10         -6.187891\n",
       "V11          5.664395\n",
       "V12         -9.854485\n",
       "V13         -0.306167\n",
       "V14        -10.691196\n",
       "V15         -0.638498\n",
       "V16         -2.041974\n",
       "V17         -1.129056\n",
       "V18          0.116453\n",
       "V19         -1.934666\n",
       "V20          0.488378\n",
       "V21          0.364514\n",
       "V22         -0.608057\n",
       "V23         -0.539528\n",
       "V24          0.128940\n",
       "V25          1.488481\n",
       "V26          0.507963\n",
       "V27          0.735822\n",
       "V28          0.513574\n",
       "Amount       1.000000\n",
       "Class        1.000000\n",
       "Name: 5, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[: , : -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126911</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102339</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430022</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171608</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0   406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "1   472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "2  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "3  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "4  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
       "0 -2.537387  1.391657 -2.770089  ...  0.126911  0.517232 -0.035049 -0.465211   \n",
       "1  0.325574 -0.067794 -0.270953  ...  2.102339  0.661696  0.435477  1.375966   \n",
       "2  0.562320 -0.399147 -0.238253  ... -0.430022 -0.294166 -0.932391  0.172726   \n",
       "3 -3.496197 -0.248778 -0.247768  ... -0.171608  0.573574  0.176968 -0.436207   \n",
       "4  1.713445 -0.496358 -1.282858  ...  0.009061 -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.320198  0.044519  0.177840  0.261145 -0.143276    0.00  \n",
       "1 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00  \n",
       "2 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93  \n",
       "3 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00  \n",
       "4 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787, 30, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(32, 2, activation='relu', input_shape= X_train[0].shape),\n",
    "    BatchNormalization(),\n",
    "    MaxPool1D(2),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv1D(64, 2, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool1D(2),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64, activation= 'relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(1, activation= 'sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 29, 32)            96        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 29, 32)            128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 13, 64)            4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 13, 64)            256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 6, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 29,345\n",
      "Trainable params: 29,153\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001), loss = 'binary_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 787 samples, validate on 197 samples\n",
      "Epoch 1/500\n",
      "787/787 [==============================] - 0s 624us/sample - loss: 0.6582 - acc: 0.7548 - val_loss: 0.5298 - val_acc: 0.8883\n",
      "Epoch 2/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.3896 - acc: 0.8640 - val_loss: 0.4797 - val_acc: 0.9391\n",
      "Epoch 3/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.4006 - acc: 0.8806 - val_loss: 0.4162 - val_acc: 0.9391\n",
      "Epoch 4/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.3158 - acc: 0.8996 - val_loss: 0.3551 - val_acc: 0.9492\n",
      "Epoch 5/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.3025 - acc: 0.8971 - val_loss: 0.2977 - val_acc: 0.9594\n",
      "Epoch 6/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.2354 - acc: 0.9212 - val_loss: 0.2520 - val_acc: 0.9594\n",
      "Epoch 7/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.2609 - acc: 0.9174 - val_loss: 0.2176 - val_acc: 0.9543\n",
      "Epoch 8/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.2880 - acc: 0.9034 - val_loss: 0.1925 - val_acc: 0.9594\n",
      "Epoch 9/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.2132 - acc: 0.9136 - val_loss: 0.1710 - val_acc: 0.9594\n",
      "Epoch 10/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.2600 - acc: 0.9136 - val_loss: 0.1562 - val_acc: 0.9594\n",
      "Epoch 11/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.2437 - acc: 0.9174 - val_loss: 0.1443 - val_acc: 0.9543\n",
      "Epoch 12/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.2234 - acc: 0.9263 - val_loss: 0.1420 - val_acc: 0.9594\n",
      "Epoch 13/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.2128 - acc: 0.9161 - val_loss: 0.1376 - val_acc: 0.9594\n",
      "Epoch 14/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.2163 - acc: 0.9238 - val_loss: 0.1327 - val_acc: 0.9543\n",
      "Epoch 15/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.2280 - acc: 0.9174 - val_loss: 0.1312 - val_acc: 0.9543\n",
      "Epoch 16/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.1951 - acc: 0.9301 - val_loss: 0.1299 - val_acc: 0.9543\n",
      "Epoch 17/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.2102 - acc: 0.9225 - val_loss: 0.1304 - val_acc: 0.9543\n",
      "Epoch 18/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.2211 - acc: 0.9111 - val_loss: 0.1278 - val_acc: 0.9594\n",
      "Epoch 19/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.2049 - acc: 0.9288 - val_loss: 0.1278 - val_acc: 0.9543\n",
      "Epoch 20/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.2105 - acc: 0.9276 - val_loss: 0.1263 - val_acc: 0.9543\n",
      "Epoch 21/500\n",
      "787/787 [==============================] - 0s 148us/sample - loss: 0.1837 - acc: 0.9301 - val_loss: 0.1243 - val_acc: 0.9543\n",
      "Epoch 22/500\n",
      "787/787 [==============================] - 0s 166us/sample - loss: 0.1843 - acc: 0.9390 - val_loss: 0.1231 - val_acc: 0.9594\n",
      "Epoch 23/500\n",
      "787/787 [==============================] - 0s 155us/sample - loss: 0.1825 - acc: 0.9403 - val_loss: 0.1225 - val_acc: 0.9594\n",
      "Epoch 24/500\n",
      "787/787 [==============================] - 0s 154us/sample - loss: 0.1764 - acc: 0.9327 - val_loss: 0.1240 - val_acc: 0.9594\n",
      "Epoch 25/500\n",
      "787/787 [==============================] - 0s 146us/sample - loss: 0.1970 - acc: 0.9301 - val_loss: 0.1258 - val_acc: 0.9543\n",
      "Epoch 26/500\n",
      "787/787 [==============================] - 0s 148us/sample - loss: 0.1956 - acc: 0.9276 - val_loss: 0.1292 - val_acc: 0.9543\n",
      "Epoch 27/500\n",
      "787/787 [==============================] - 0s 151us/sample - loss: 0.1699 - acc: 0.9377 - val_loss: 0.1290 - val_acc: 0.9543\n",
      "Epoch 28/500\n",
      "787/787 [==============================] - 0s 147us/sample - loss: 0.1862 - acc: 0.9466 - val_loss: 0.1295 - val_acc: 0.9492\n",
      "Epoch 29/500\n",
      "787/787 [==============================] - 0s 140us/sample - loss: 0.1683 - acc: 0.9365 - val_loss: 0.1286 - val_acc: 0.9543\n",
      "Epoch 30/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.1758 - acc: 0.9365 - val_loss: 0.1280 - val_acc: 0.9492\n",
      "Epoch 31/500\n",
      "787/787 [==============================] - 0s 138us/sample - loss: 0.1578 - acc: 0.9416 - val_loss: 0.1320 - val_acc: 0.9543\n",
      "Epoch 32/500\n",
      "787/787 [==============================] - 0s 140us/sample - loss: 0.1890 - acc: 0.9390 - val_loss: 0.1350 - val_acc: 0.9442\n",
      "Epoch 33/500\n",
      "787/787 [==============================] - 0s 137us/sample - loss: 0.1858 - acc: 0.9352 - val_loss: 0.1315 - val_acc: 0.9543\n",
      "Epoch 34/500\n",
      "787/787 [==============================] - 0s 139us/sample - loss: 0.1713 - acc: 0.9416 - val_loss: 0.1325 - val_acc: 0.9543\n",
      "Epoch 35/500\n",
      "787/787 [==============================] - 0s 139us/sample - loss: 0.1416 - acc: 0.9454 - val_loss: 0.1310 - val_acc: 0.9543\n",
      "Epoch 36/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.1672 - acc: 0.9403 - val_loss: 0.1302 - val_acc: 0.9543\n",
      "Epoch 37/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.2010 - acc: 0.9327 - val_loss: 0.1318 - val_acc: 0.9543\n",
      "Epoch 38/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.1801 - acc: 0.9352 - val_loss: 0.1319 - val_acc: 0.9492\n",
      "Epoch 39/500\n",
      "787/787 [==============================] - 0s 116us/sample - loss: 0.1637 - acc: 0.9352 - val_loss: 0.1296 - val_acc: 0.9543\n",
      "Epoch 40/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1653 - acc: 0.9441 - val_loss: 0.1307 - val_acc: 0.9543\n",
      "Epoch 41/500\n",
      "787/787 [==============================] - 0s 132us/sample - loss: 0.1614 - acc: 0.9377 - val_loss: 0.1283 - val_acc: 0.9492\n",
      "Epoch 42/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1615 - acc: 0.9416 - val_loss: 0.1255 - val_acc: 0.9543\n",
      "Epoch 43/500\n",
      "787/787 [==============================] - 0s 116us/sample - loss: 0.1505 - acc: 0.9454 - val_loss: 0.1261 - val_acc: 0.9543\n",
      "Epoch 44/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1914 - acc: 0.9339 - val_loss: 0.1294 - val_acc: 0.9594\n",
      "Epoch 45/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.1645 - acc: 0.9416 - val_loss: 0.1312 - val_acc: 0.9543\n",
      "Epoch 46/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1705 - acc: 0.9504 - val_loss: 0.1330 - val_acc: 0.9543\n",
      "Epoch 47/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.1605 - acc: 0.9454 - val_loss: 0.1346 - val_acc: 0.9543\n",
      "Epoch 48/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.1529 - acc: 0.9466 - val_loss: 0.1359 - val_acc: 0.9543\n",
      "Epoch 49/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.1499 - acc: 0.9441 - val_loss: 0.1373 - val_acc: 0.9594\n",
      "Epoch 50/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.1367 - acc: 0.9504 - val_loss: 0.1379 - val_acc: 0.9594\n",
      "Epoch 51/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.1591 - acc: 0.9327 - val_loss: 0.1380 - val_acc: 0.9543\n",
      "Epoch 52/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1495 - acc: 0.9466 - val_loss: 0.1411 - val_acc: 0.9543\n",
      "Epoch 53/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.1492 - acc: 0.9416 - val_loss: 0.1356 - val_acc: 0.9543\n",
      "Epoch 54/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.1512 - acc: 0.9428 - val_loss: 0.1354 - val_acc: 0.9543\n",
      "Epoch 55/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1509 - acc: 0.9441 - val_loss: 0.1354 - val_acc: 0.9543\n",
      "Epoch 56/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.1450 - acc: 0.9555 - val_loss: 0.1378 - val_acc: 0.9645\n",
      "Epoch 57/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.1407 - acc: 0.9479 - val_loss: 0.1407 - val_acc: 0.9645\n",
      "Epoch 58/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1371 - acc: 0.9479 - val_loss: 0.1406 - val_acc: 0.9543\n",
      "Epoch 59/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.1498 - acc: 0.9428 - val_loss: 0.1421 - val_acc: 0.9594\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1422 - acc: 0.9454 - val_loss: 0.1398 - val_acc: 0.9594\n",
      "Epoch 61/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.1499 - acc: 0.9530 - val_loss: 0.1402 - val_acc: 0.9645\n",
      "Epoch 62/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1290 - acc: 0.9517 - val_loss: 0.1388 - val_acc: 0.9594\n",
      "Epoch 63/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1328 - acc: 0.9466 - val_loss: 0.1440 - val_acc: 0.9594\n",
      "Epoch 64/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1341 - acc: 0.9479 - val_loss: 0.1461 - val_acc: 0.9645\n",
      "Epoch 65/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1369 - acc: 0.9479 - val_loss: 0.1475 - val_acc: 0.9594\n",
      "Epoch 66/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1275 - acc: 0.9543 - val_loss: 0.1450 - val_acc: 0.9594\n",
      "Epoch 67/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.1336 - acc: 0.9619 - val_loss: 0.1452 - val_acc: 0.9594\n",
      "Epoch 68/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.1538 - acc: 0.9441 - val_loss: 0.1507 - val_acc: 0.9543\n",
      "Epoch 69/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1447 - acc: 0.9517 - val_loss: 0.1451 - val_acc: 0.9594\n",
      "Epoch 70/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.1541 - acc: 0.9365 - val_loss: 0.1420 - val_acc: 0.9645\n",
      "Epoch 71/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.1185 - acc: 0.9517 - val_loss: 0.1454 - val_acc: 0.9594\n",
      "Epoch 72/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.1300 - acc: 0.9466 - val_loss: 0.1476 - val_acc: 0.9594\n",
      "Epoch 73/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.1315 - acc: 0.9581 - val_loss: 0.1480 - val_acc: 0.9594\n",
      "Epoch 74/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1383 - acc: 0.9504 - val_loss: 0.1446 - val_acc: 0.9594\n",
      "Epoch 75/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1131 - acc: 0.9504 - val_loss: 0.1469 - val_acc: 0.9645\n",
      "Epoch 76/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.1303 - acc: 0.9530 - val_loss: 0.1447 - val_acc: 0.9543\n",
      "Epoch 77/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.1281 - acc: 0.9593 - val_loss: 0.1449 - val_acc: 0.9645\n",
      "Epoch 78/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1290 - acc: 0.9543 - val_loss: 0.1486 - val_acc: 0.9645\n",
      "Epoch 79/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0983 - acc: 0.9632 - val_loss: 0.1530 - val_acc: 0.9645\n",
      "Epoch 80/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.1341 - acc: 0.9543 - val_loss: 0.1533 - val_acc: 0.9645\n",
      "Epoch 81/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.1345 - acc: 0.9593 - val_loss: 0.1571 - val_acc: 0.9645\n",
      "Epoch 82/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.1322 - acc: 0.9555 - val_loss: 0.1554 - val_acc: 0.9594\n",
      "Epoch 83/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.1039 - acc: 0.9581 - val_loss: 0.1563 - val_acc: 0.9594\n",
      "Epoch 84/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1106 - acc: 0.9568 - val_loss: 0.1575 - val_acc: 0.9645\n",
      "Epoch 85/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1211 - acc: 0.9504 - val_loss: 0.1523 - val_acc: 0.9594\n",
      "Epoch 86/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1126 - acc: 0.9517 - val_loss: 0.1576 - val_acc: 0.9594\n",
      "Epoch 87/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.1203 - acc: 0.9593 - val_loss: 0.1564 - val_acc: 0.9645\n",
      "Epoch 88/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1235 - acc: 0.9581 - val_loss: 0.1590 - val_acc: 0.9645\n",
      "Epoch 89/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.1179 - acc: 0.9517 - val_loss: 0.1593 - val_acc: 0.9594\n",
      "Epoch 90/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.1150 - acc: 0.9606 - val_loss: 0.1608 - val_acc: 0.9645\n",
      "Epoch 91/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.1236 - acc: 0.9543 - val_loss: 0.1630 - val_acc: 0.9594\n",
      "Epoch 92/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.1195 - acc: 0.9593 - val_loss: 0.1625 - val_acc: 0.9594\n",
      "Epoch 93/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.1103 - acc: 0.9593 - val_loss: 0.1638 - val_acc: 0.9645\n",
      "Epoch 94/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.1055 - acc: 0.9568 - val_loss: 0.1642 - val_acc: 0.9645\n",
      "Epoch 95/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.1159 - acc: 0.9555 - val_loss: 0.1662 - val_acc: 0.9594\n",
      "Epoch 96/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0948 - acc: 0.9606 - val_loss: 0.1710 - val_acc: 0.9543\n",
      "Epoch 97/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.1053 - acc: 0.9530 - val_loss: 0.1660 - val_acc: 0.9594\n",
      "Epoch 98/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.1129 - acc: 0.9543 - val_loss: 0.1677 - val_acc: 0.9594\n",
      "Epoch 99/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1144 - acc: 0.9530 - val_loss: 0.1697 - val_acc: 0.9645\n",
      "Epoch 100/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0982 - acc: 0.9657 - val_loss: 0.1723 - val_acc: 0.9645\n",
      "Epoch 101/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.1104 - acc: 0.9530 - val_loss: 0.1747 - val_acc: 0.9645\n",
      "Epoch 102/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.1192 - acc: 0.9581 - val_loss: 0.1762 - val_acc: 0.9645\n",
      "Epoch 103/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1207 - acc: 0.9581 - val_loss: 0.1755 - val_acc: 0.9645\n",
      "Epoch 104/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0967 - acc: 0.9644 - val_loss: 0.1764 - val_acc: 0.9645\n",
      "Epoch 105/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0963 - acc: 0.9657 - val_loss: 0.1733 - val_acc: 0.9594\n",
      "Epoch 106/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0949 - acc: 0.9632 - val_loss: 0.1729 - val_acc: 0.9594\n",
      "Epoch 107/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1217 - acc: 0.9543 - val_loss: 0.1704 - val_acc: 0.9594\n",
      "Epoch 108/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1090 - acc: 0.9543 - val_loss: 0.1704 - val_acc: 0.9594\n",
      "Epoch 109/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0943 - acc: 0.9593 - val_loss: 0.1714 - val_acc: 0.9594\n",
      "Epoch 110/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1059 - acc: 0.9555 - val_loss: 0.1709 - val_acc: 0.9594\n",
      "Epoch 111/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0972 - acc: 0.9593 - val_loss: 0.1716 - val_acc: 0.9594\n",
      "Epoch 112/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0917 - acc: 0.9632 - val_loss: 0.1716 - val_acc: 0.9594\n",
      "Epoch 113/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1421 - acc: 0.9581 - val_loss: 0.1654 - val_acc: 0.9645\n",
      "Epoch 114/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1060 - acc: 0.9619 - val_loss: 0.1698 - val_acc: 0.9645\n",
      "Epoch 115/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0980 - acc: 0.9670 - val_loss: 0.1628 - val_acc: 0.9645\n",
      "Epoch 116/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1143 - acc: 0.9543 - val_loss: 0.1617 - val_acc: 0.9594\n",
      "Epoch 117/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.1067 - acc: 0.9568 - val_loss: 0.1659 - val_acc: 0.9594\n",
      "Epoch 118/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.1095 - acc: 0.9568 - val_loss: 0.1632 - val_acc: 0.9594\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0981 - acc: 0.9593 - val_loss: 0.1557 - val_acc: 0.9594\n",
      "Epoch 120/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0945 - acc: 0.9708 - val_loss: 0.1561 - val_acc: 0.9695\n",
      "Epoch 121/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0832 - acc: 0.9682 - val_loss: 0.1598 - val_acc: 0.9645\n",
      "Epoch 122/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0957 - acc: 0.9632 - val_loss: 0.1645 - val_acc: 0.9645\n",
      "Epoch 123/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0900 - acc: 0.9657 - val_loss: 0.1701 - val_acc: 0.9645\n",
      "Epoch 124/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0987 - acc: 0.9619 - val_loss: 0.1752 - val_acc: 0.9645\n",
      "Epoch 125/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0922 - acc: 0.9670 - val_loss: 0.1728 - val_acc: 0.9645\n",
      "Epoch 126/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0955 - acc: 0.9619 - val_loss: 0.1738 - val_acc: 0.9594\n",
      "Epoch 127/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.1172 - acc: 0.9581 - val_loss: 0.1700 - val_acc: 0.9594\n",
      "Epoch 128/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0888 - acc: 0.9632 - val_loss: 0.1704 - val_acc: 0.9594\n",
      "Epoch 129/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0779 - acc: 0.9708 - val_loss: 0.1707 - val_acc: 0.9594\n",
      "Epoch 130/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.1131 - acc: 0.9644 - val_loss: 0.1722 - val_acc: 0.9645\n",
      "Epoch 131/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0900 - acc: 0.9708 - val_loss: 0.1673 - val_acc: 0.9645\n",
      "Epoch 132/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.1209 - acc: 0.9517 - val_loss: 0.1667 - val_acc: 0.9645\n",
      "Epoch 133/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0865 - acc: 0.9644 - val_loss: 0.1717 - val_acc: 0.9594\n",
      "Epoch 134/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0792 - acc: 0.9632 - val_loss: 0.1744 - val_acc: 0.9645\n",
      "Epoch 135/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0644 - acc: 0.9759 - val_loss: 0.1711 - val_acc: 0.9645\n",
      "Epoch 136/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0990 - acc: 0.9593 - val_loss: 0.1713 - val_acc: 0.9645\n",
      "Epoch 137/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0741 - acc: 0.9759 - val_loss: 0.1692 - val_acc: 0.9645\n",
      "Epoch 138/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0750 - acc: 0.9682 - val_loss: 0.1703 - val_acc: 0.9594\n",
      "Epoch 139/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0976 - acc: 0.9619 - val_loss: 0.1709 - val_acc: 0.9645\n",
      "Epoch 140/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0876 - acc: 0.9606 - val_loss: 0.1692 - val_acc: 0.9645\n",
      "Epoch 141/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0993 - acc: 0.9619 - val_loss: 0.1712 - val_acc: 0.9645\n",
      "Epoch 142/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0855 - acc: 0.9670 - val_loss: 0.1740 - val_acc: 0.9645\n",
      "Epoch 143/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0904 - acc: 0.9682 - val_loss: 0.1742 - val_acc: 0.9645\n",
      "Epoch 144/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0948 - acc: 0.9657 - val_loss: 0.1715 - val_acc: 0.9645\n",
      "Epoch 145/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.1005 - acc: 0.9593 - val_loss: 0.1611 - val_acc: 0.9645\n",
      "Epoch 146/500\n",
      "787/787 [==============================] - 0s 116us/sample - loss: 0.0981 - acc: 0.9695 - val_loss: 0.1665 - val_acc: 0.9695\n",
      "Epoch 147/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0991 - acc: 0.9695 - val_loss: 0.1681 - val_acc: 0.9695\n",
      "Epoch 148/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0747 - acc: 0.9771 - val_loss: 0.1704 - val_acc: 0.9695\n",
      "Epoch 149/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0913 - acc: 0.9708 - val_loss: 0.1719 - val_acc: 0.9645\n",
      "Epoch 150/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0766 - acc: 0.9682 - val_loss: 0.1703 - val_acc: 0.9645\n",
      "Epoch 151/500\n",
      "787/787 [==============================] - 0s 136us/sample - loss: 0.0847 - acc: 0.9657 - val_loss: 0.1703 - val_acc: 0.9645\n",
      "Epoch 152/500\n",
      "787/787 [==============================] - 0s 143us/sample - loss: 0.0746 - acc: 0.9695 - val_loss: 0.1652 - val_acc: 0.9594\n",
      "Epoch 153/500\n",
      "787/787 [==============================] - 0s 136us/sample - loss: 0.0894 - acc: 0.9632 - val_loss: 0.1648 - val_acc: 0.9594\n",
      "Epoch 154/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0932 - acc: 0.9644 - val_loss: 0.1631 - val_acc: 0.9695\n",
      "Epoch 155/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0742 - acc: 0.9682 - val_loss: 0.1602 - val_acc: 0.9645\n",
      "Epoch 156/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0892 - acc: 0.9695 - val_loss: 0.1622 - val_acc: 0.9645\n",
      "Epoch 157/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0951 - acc: 0.9593 - val_loss: 0.1639 - val_acc: 0.9645\n",
      "Epoch 158/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0939 - acc: 0.9708 - val_loss: 0.1642 - val_acc: 0.9645\n",
      "Epoch 159/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0753 - acc: 0.9733 - val_loss: 0.1711 - val_acc: 0.9695\n",
      "Epoch 160/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0554 - acc: 0.9784 - val_loss: 0.1763 - val_acc: 0.9695\n",
      "Epoch 161/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0789 - acc: 0.9670 - val_loss: 0.1708 - val_acc: 0.9594\n",
      "Epoch 162/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0844 - acc: 0.9695 - val_loss: 0.1664 - val_acc: 0.9645\n",
      "Epoch 163/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0756 - acc: 0.9695 - val_loss: 0.1675 - val_acc: 0.9645\n",
      "Epoch 164/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0658 - acc: 0.9771 - val_loss: 0.1761 - val_acc: 0.9645\n",
      "Epoch 165/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0781 - acc: 0.9720 - val_loss: 0.1758 - val_acc: 0.9645\n",
      "Epoch 166/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0916 - acc: 0.9708 - val_loss: 0.1782 - val_acc: 0.9645\n",
      "Epoch 167/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0944 - acc: 0.9695 - val_loss: 0.1797 - val_acc: 0.9594\n",
      "Epoch 168/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0898 - acc: 0.9657 - val_loss: 0.1813 - val_acc: 0.9695\n",
      "Epoch 169/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0819 - acc: 0.9682 - val_loss: 0.1808 - val_acc: 0.9695\n",
      "Epoch 170/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.1016 - acc: 0.9606 - val_loss: 0.1802 - val_acc: 0.9645\n",
      "Epoch 171/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0638 - acc: 0.9771 - val_loss: 0.1887 - val_acc: 0.9594\n",
      "Epoch 172/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0720 - acc: 0.9720 - val_loss: 0.1895 - val_acc: 0.9594\n",
      "Epoch 173/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0626 - acc: 0.9759 - val_loss: 0.1881 - val_acc: 0.9594\n",
      "Epoch 174/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0694 - acc: 0.9746 - val_loss: 0.1870 - val_acc: 0.9594\n",
      "Epoch 175/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0901 - acc: 0.9720 - val_loss: 0.1804 - val_acc: 0.9594\n",
      "Epoch 176/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0661 - acc: 0.9670 - val_loss: 0.1817 - val_acc: 0.9594\n",
      "Epoch 177/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0825 - acc: 0.9657 - val_loss: 0.1791 - val_acc: 0.9645\n",
      "Epoch 178/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0817 - acc: 0.9682 - val_loss: 0.1769 - val_acc: 0.9645\n",
      "Epoch 179/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0906 - acc: 0.9632 - val_loss: 0.1746 - val_acc: 0.9594\n",
      "Epoch 180/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0722 - acc: 0.9695 - val_loss: 0.1730 - val_acc: 0.9594\n",
      "Epoch 181/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0928 - acc: 0.9670 - val_loss: 0.1700 - val_acc: 0.9594\n",
      "Epoch 182/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0826 - acc: 0.9695 - val_loss: 0.1739 - val_acc: 0.9594\n",
      "Epoch 183/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0701 - acc: 0.9708 - val_loss: 0.1750 - val_acc: 0.9594\n",
      "Epoch 184/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0676 - acc: 0.9759 - val_loss: 0.1747 - val_acc: 0.9594\n",
      "Epoch 185/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0896 - acc: 0.9657 - val_loss: 0.1722 - val_acc: 0.9594\n",
      "Epoch 186/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0691 - acc: 0.9809 - val_loss: 0.1721 - val_acc: 0.9594\n",
      "Epoch 187/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0672 - acc: 0.9682 - val_loss: 0.1734 - val_acc: 0.9594\n",
      "Epoch 188/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0863 - acc: 0.9657 - val_loss: 0.1713 - val_acc: 0.9594\n",
      "Epoch 189/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0643 - acc: 0.9682 - val_loss: 0.1741 - val_acc: 0.9543\n",
      "Epoch 190/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0729 - acc: 0.9720 - val_loss: 0.1748 - val_acc: 0.9594\n",
      "Epoch 191/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0660 - acc: 0.9720 - val_loss: 0.1787 - val_acc: 0.9594\n",
      "Epoch 192/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0611 - acc: 0.9759 - val_loss: 0.1818 - val_acc: 0.9594\n",
      "Epoch 193/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0799 - acc: 0.9708 - val_loss: 0.1784 - val_acc: 0.9594\n",
      "Epoch 194/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0847 - acc: 0.9644 - val_loss: 0.1840 - val_acc: 0.9594\n",
      "Epoch 195/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0602 - acc: 0.9797 - val_loss: 0.1878 - val_acc: 0.9594\n",
      "Epoch 196/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0910 - acc: 0.9708 - val_loss: 0.1885 - val_acc: 0.9594\n",
      "Epoch 197/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0788 - acc: 0.9695 - val_loss: 0.1878 - val_acc: 0.9594\n",
      "Epoch 198/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0734 - acc: 0.9670 - val_loss: 0.1902 - val_acc: 0.9594\n",
      "Epoch 199/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0643 - acc: 0.9759 - val_loss: 0.1836 - val_acc: 0.9594\n",
      "Epoch 200/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0773 - acc: 0.9720 - val_loss: 0.1835 - val_acc: 0.9594\n",
      "Epoch 201/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0605 - acc: 0.9771 - val_loss: 0.1824 - val_acc: 0.9645\n",
      "Epoch 202/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0692 - acc: 0.9759 - val_loss: 0.1851 - val_acc: 0.9594\n",
      "Epoch 203/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0703 - acc: 0.9695 - val_loss: 0.1835 - val_acc: 0.9645\n",
      "Epoch 204/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0698 - acc: 0.9746 - val_loss: 0.1877 - val_acc: 0.9645\n",
      "Epoch 205/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0551 - acc: 0.9848 - val_loss: 0.1802 - val_acc: 0.9594\n",
      "Epoch 206/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0687 - acc: 0.9771 - val_loss: 0.1821 - val_acc: 0.9594\n",
      "Epoch 207/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0672 - acc: 0.9708 - val_loss: 0.1843 - val_acc: 0.9594\n",
      "Epoch 208/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0618 - acc: 0.9759 - val_loss: 0.1845 - val_acc: 0.9594\n",
      "Epoch 209/500\n",
      "787/787 [==============================] - 0s 132us/sample - loss: 0.0658 - acc: 0.9733 - val_loss: 0.1828 - val_acc: 0.9594\n",
      "Epoch 210/500\n",
      "787/787 [==============================] - 0s 137us/sample - loss: 0.0883 - acc: 0.9670 - val_loss: 0.1778 - val_acc: 0.9594\n",
      "Epoch 211/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0569 - acc: 0.9784 - val_loss: 0.1877 - val_acc: 0.9594\n",
      "Epoch 212/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0588 - acc: 0.9797 - val_loss: 0.1873 - val_acc: 0.9594\n",
      "Epoch 213/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0632 - acc: 0.9733 - val_loss: 0.1912 - val_acc: 0.9594\n",
      "Epoch 214/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0651 - acc: 0.9759 - val_loss: 0.1951 - val_acc: 0.9492\n",
      "Epoch 215/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0579 - acc: 0.9835 - val_loss: 0.1995 - val_acc: 0.9594\n",
      "Epoch 216/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0643 - acc: 0.9733 - val_loss: 0.1904 - val_acc: 0.9594\n",
      "Epoch 217/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0621 - acc: 0.9759 - val_loss: 0.1846 - val_acc: 0.9594\n",
      "Epoch 218/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0681 - acc: 0.9771 - val_loss: 0.1893 - val_acc: 0.9594\n",
      "Epoch 219/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0784 - acc: 0.9784 - val_loss: 0.1965 - val_acc: 0.9543\n",
      "Epoch 220/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0702 - acc: 0.9695 - val_loss: 0.2040 - val_acc: 0.9594\n",
      "Epoch 221/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0669 - acc: 0.9720 - val_loss: 0.2118 - val_acc: 0.9594\n",
      "Epoch 222/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0540 - acc: 0.9797 - val_loss: 0.2084 - val_acc: 0.9543\n",
      "Epoch 223/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0623 - acc: 0.9746 - val_loss: 0.2059 - val_acc: 0.9594\n",
      "Epoch 224/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0512 - acc: 0.9835 - val_loss: 0.2035 - val_acc: 0.9594\n",
      "Epoch 225/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0548 - acc: 0.9822 - val_loss: 0.2108 - val_acc: 0.9594\n",
      "Epoch 226/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0625 - acc: 0.9797 - val_loss: 0.2064 - val_acc: 0.9492\n",
      "Epoch 227/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0504 - acc: 0.9822 - val_loss: 0.2133 - val_acc: 0.9594\n",
      "Epoch 228/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0624 - acc: 0.9771 - val_loss: 0.2122 - val_acc: 0.9594\n",
      "Epoch 229/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0640 - acc: 0.9746 - val_loss: 0.2144 - val_acc: 0.9594\n",
      "Epoch 230/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0642 - acc: 0.9784 - val_loss: 0.2094 - val_acc: 0.9594\n",
      "Epoch 231/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0696 - acc: 0.9746 - val_loss: 0.2101 - val_acc: 0.9594\n",
      "Epoch 232/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0692 - acc: 0.9733 - val_loss: 0.2070 - val_acc: 0.9594\n",
      "Epoch 233/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0671 - acc: 0.9695 - val_loss: 0.2074 - val_acc: 0.9594\n",
      "Epoch 234/500\n",
      "787/787 [==============================] - 0s 139us/sample - loss: 0.0577 - acc: 0.9759 - val_loss: 0.2223 - val_acc: 0.9594\n",
      "Epoch 235/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0756 - acc: 0.9682 - val_loss: 0.2223 - val_acc: 0.9594\n",
      "Epoch 236/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0676 - acc: 0.9733 - val_loss: 0.2243 - val_acc: 0.9594\n",
      "Epoch 237/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0675 - acc: 0.9733 - val_loss: 0.2192 - val_acc: 0.9594\n",
      "Epoch 238/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0787 - acc: 0.9695 - val_loss: 0.2139 - val_acc: 0.9594\n",
      "Epoch 239/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0630 - acc: 0.9720 - val_loss: 0.2050 - val_acc: 0.9594\n",
      "Epoch 240/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0500 - acc: 0.9822 - val_loss: 0.1971 - val_acc: 0.9594\n",
      "Epoch 241/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0696 - acc: 0.9733 - val_loss: 0.1947 - val_acc: 0.9594\n",
      "Epoch 242/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0609 - acc: 0.9784 - val_loss: 0.2010 - val_acc: 0.9594\n",
      "Epoch 243/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0737 - acc: 0.9695 - val_loss: 0.2071 - val_acc: 0.9594\n",
      "Epoch 244/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0491 - acc: 0.9797 - val_loss: 0.2127 - val_acc: 0.9594\n",
      "Epoch 245/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0574 - acc: 0.9809 - val_loss: 0.2057 - val_acc: 0.9594\n",
      "Epoch 246/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0625 - acc: 0.9771 - val_loss: 0.2055 - val_acc: 0.9594\n",
      "Epoch 247/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0703 - acc: 0.9746 - val_loss: 0.2024 - val_acc: 0.9594\n",
      "Epoch 248/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0625 - acc: 0.9746 - val_loss: 0.2005 - val_acc: 0.9594\n",
      "Epoch 249/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0568 - acc: 0.9746 - val_loss: 0.2084 - val_acc: 0.9594\n",
      "Epoch 250/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0649 - acc: 0.9784 - val_loss: 0.2357 - val_acc: 0.9645\n",
      "Epoch 251/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0806 - acc: 0.9708 - val_loss: 0.2059 - val_acc: 0.9594\n",
      "Epoch 252/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0585 - acc: 0.9720 - val_loss: 0.2113 - val_acc: 0.9645\n",
      "Epoch 253/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0659 - acc: 0.9822 - val_loss: 0.2160 - val_acc: 0.9594\n",
      "Epoch 254/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0614 - acc: 0.9784 - val_loss: 0.2176 - val_acc: 0.9594\n",
      "Epoch 255/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0483 - acc: 0.9771 - val_loss: 0.2312 - val_acc: 0.9594\n",
      "Epoch 256/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0580 - acc: 0.9746 - val_loss: 0.2249 - val_acc: 0.9594\n",
      "Epoch 257/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0470 - acc: 0.9835 - val_loss: 0.2188 - val_acc: 0.9594\n",
      "Epoch 258/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0554 - acc: 0.9771 - val_loss: 0.2100 - val_acc: 0.9594\n",
      "Epoch 259/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0734 - acc: 0.9746 - val_loss: 0.1986 - val_acc: 0.9594\n",
      "Epoch 260/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0767 - acc: 0.9746 - val_loss: 0.1958 - val_acc: 0.9594\n",
      "Epoch 261/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0573 - acc: 0.9746 - val_loss: 0.1936 - val_acc: 0.9594\n",
      "Epoch 262/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0552 - acc: 0.9784 - val_loss: 0.2002 - val_acc: 0.9594\n",
      "Epoch 263/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0692 - acc: 0.9746 - val_loss: 0.1978 - val_acc: 0.9594\n",
      "Epoch 264/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0536 - acc: 0.9797 - val_loss: 0.1966 - val_acc: 0.9594\n",
      "Epoch 265/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0629 - acc: 0.9746 - val_loss: 0.1991 - val_acc: 0.9594\n",
      "Epoch 266/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0471 - acc: 0.9797 - val_loss: 0.2067 - val_acc: 0.9594\n",
      "Epoch 267/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0590 - acc: 0.9835 - val_loss: 0.2051 - val_acc: 0.9594\n",
      "Epoch 268/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0549 - acc: 0.9797 - val_loss: 0.2104 - val_acc: 0.9645\n",
      "Epoch 269/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0717 - acc: 0.9708 - val_loss: 0.2125 - val_acc: 0.9594\n",
      "Epoch 270/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0645 - acc: 0.9682 - val_loss: 0.2022 - val_acc: 0.9594\n",
      "Epoch 271/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0519 - acc: 0.9771 - val_loss: 0.2048 - val_acc: 0.9594\n",
      "Epoch 272/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0564 - acc: 0.9797 - val_loss: 0.1953 - val_acc: 0.9645\n",
      "Epoch 273/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0679 - acc: 0.9746 - val_loss: 0.1969 - val_acc: 0.9594\n",
      "Epoch 274/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0638 - acc: 0.9733 - val_loss: 0.2008 - val_acc: 0.9594\n",
      "Epoch 275/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0566 - acc: 0.9759 - val_loss: 0.1992 - val_acc: 0.9594\n",
      "Epoch 276/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0584 - acc: 0.9784 - val_loss: 0.2010 - val_acc: 0.9645\n",
      "Epoch 277/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0676 - acc: 0.9784 - val_loss: 0.1912 - val_acc: 0.9594\n",
      "Epoch 278/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0831 - acc: 0.9720 - val_loss: 0.1996 - val_acc: 0.9645\n",
      "Epoch 279/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0492 - acc: 0.9835 - val_loss: 0.1931 - val_acc: 0.9645\n",
      "Epoch 280/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0642 - acc: 0.9746 - val_loss: 0.1847 - val_acc: 0.9645\n",
      "Epoch 281/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0550 - acc: 0.9848 - val_loss: 0.1837 - val_acc: 0.9645\n",
      "Epoch 282/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0598 - acc: 0.9771 - val_loss: 0.1940 - val_acc: 0.9645\n",
      "Epoch 283/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0538 - acc: 0.9822 - val_loss: 0.1849 - val_acc: 0.9645\n",
      "Epoch 284/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0510 - acc: 0.9797 - val_loss: 0.2014 - val_acc: 0.9645\n",
      "Epoch 285/500\n",
      "787/787 [==============================] - 0s 132us/sample - loss: 0.0582 - acc: 0.9771 - val_loss: 0.2013 - val_acc: 0.9645\n",
      "Epoch 286/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0493 - acc: 0.9784 - val_loss: 0.2079 - val_acc: 0.9645\n",
      "Epoch 287/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0585 - acc: 0.9746 - val_loss: 0.2235 - val_acc: 0.9645\n",
      "Epoch 288/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0547 - acc: 0.9809 - val_loss: 0.2139 - val_acc: 0.9543\n",
      "Epoch 289/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0660 - acc: 0.9759 - val_loss: 0.2203 - val_acc: 0.9645\n",
      "Epoch 290/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0524 - acc: 0.9746 - val_loss: 0.2148 - val_acc: 0.9594\n",
      "Epoch 291/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0382 - acc: 0.9873 - val_loss: 0.2223 - val_acc: 0.9594\n",
      "Epoch 292/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0545 - acc: 0.9759 - val_loss: 0.2278 - val_acc: 0.9645\n",
      "Epoch 293/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0423 - acc: 0.9860 - val_loss: 0.2223 - val_acc: 0.9594\n",
      "Epoch 294/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0568 - acc: 0.9797 - val_loss: 0.2200 - val_acc: 0.9594\n",
      "Epoch 295/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0706 - acc: 0.9797 - val_loss: 0.2137 - val_acc: 0.9594\n",
      "Epoch 296/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0638 - acc: 0.9797 - val_loss: 0.2067 - val_acc: 0.9645\n",
      "Epoch 297/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0594 - acc: 0.9809 - val_loss: 0.2046 - val_acc: 0.9594\n",
      "Epoch 298/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0575 - acc: 0.9797 - val_loss: 0.2070 - val_acc: 0.9594\n",
      "Epoch 299/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0450 - acc: 0.9860 - val_loss: 0.2117 - val_acc: 0.9645\n",
      "Epoch 300/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0468 - acc: 0.9835 - val_loss: 0.2112 - val_acc: 0.9594\n",
      "Epoch 301/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0563 - acc: 0.9848 - val_loss: 0.2076 - val_acc: 0.9594\n",
      "Epoch 302/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0559 - acc: 0.9848 - val_loss: 0.2068 - val_acc: 0.9594\n",
      "Epoch 303/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0451 - acc: 0.9835 - val_loss: 0.2014 - val_acc: 0.9594\n",
      "Epoch 304/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0435 - acc: 0.9835 - val_loss: 0.2076 - val_acc: 0.9594\n",
      "Epoch 305/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0557 - acc: 0.9759 - val_loss: 0.2004 - val_acc: 0.9594\n",
      "Epoch 306/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0521 - acc: 0.9784 - val_loss: 0.2007 - val_acc: 0.9594\n",
      "Epoch 307/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0511 - acc: 0.9797 - val_loss: 0.2053 - val_acc: 0.9594\n",
      "Epoch 308/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0539 - acc: 0.9771 - val_loss: 0.2072 - val_acc: 0.9594\n",
      "Epoch 309/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0623 - acc: 0.9759 - val_loss: 0.2100 - val_acc: 0.9594\n",
      "Epoch 310/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0469 - acc: 0.9822 - val_loss: 0.2018 - val_acc: 0.9594\n",
      "Epoch 311/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0481 - acc: 0.9848 - val_loss: 0.2175 - val_acc: 0.9594\n",
      "Epoch 312/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0499 - acc: 0.9797 - val_loss: 0.2282 - val_acc: 0.9594\n",
      "Epoch 313/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0387 - acc: 0.9860 - val_loss: 0.2244 - val_acc: 0.9594\n",
      "Epoch 314/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0522 - acc: 0.9822 - val_loss: 0.2236 - val_acc: 0.9594\n",
      "Epoch 315/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0388 - acc: 0.9809 - val_loss: 0.2136 - val_acc: 0.9645\n",
      "Epoch 316/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0584 - acc: 0.9746 - val_loss: 0.2272 - val_acc: 0.9645\n",
      "Epoch 317/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0559 - acc: 0.9797 - val_loss: 0.2381 - val_acc: 0.9645\n",
      "Epoch 318/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0518 - acc: 0.9797 - val_loss: 0.2284 - val_acc: 0.9645\n",
      "Epoch 319/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0436 - acc: 0.9835 - val_loss: 0.2324 - val_acc: 0.9594\n",
      "Epoch 320/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0442 - acc: 0.9860 - val_loss: 0.2251 - val_acc: 0.9594\n",
      "Epoch 321/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0547 - acc: 0.9809 - val_loss: 0.2247 - val_acc: 0.9594\n",
      "Epoch 322/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0353 - acc: 0.9886 - val_loss: 0.2251 - val_acc: 0.9594\n",
      "Epoch 323/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0502 - acc: 0.9822 - val_loss: 0.2169 - val_acc: 0.9594\n",
      "Epoch 324/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0414 - acc: 0.9848 - val_loss: 0.2112 - val_acc: 0.9594\n",
      "Epoch 325/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0406 - acc: 0.9848 - val_loss: 0.1980 - val_acc: 0.9594\n",
      "Epoch 326/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0500 - acc: 0.9797 - val_loss: 0.1998 - val_acc: 0.9594\n",
      "Epoch 327/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0520 - acc: 0.9784 - val_loss: 0.2005 - val_acc: 0.9594\n",
      "Epoch 328/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0448 - acc: 0.9835 - val_loss: 0.2083 - val_acc: 0.9594\n",
      "Epoch 329/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0584 - acc: 0.9759 - val_loss: 0.2170 - val_acc: 0.9594\n",
      "Epoch 330/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0409 - acc: 0.9860 - val_loss: 0.2031 - val_acc: 0.9594\n",
      "Epoch 331/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0467 - acc: 0.9835 - val_loss: 0.2188 - val_acc: 0.9594\n",
      "Epoch 332/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0640 - acc: 0.9708 - val_loss: 0.2123 - val_acc: 0.9594\n",
      "Epoch 333/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0419 - acc: 0.9873 - val_loss: 0.2055 - val_acc: 0.9594\n",
      "Epoch 334/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0670 - acc: 0.9784 - val_loss: 0.2032 - val_acc: 0.9594\n",
      "Epoch 335/500\n",
      "787/787 [==============================] - 0s 118us/sample - loss: 0.0649 - acc: 0.9746 - val_loss: 0.2120 - val_acc: 0.9594\n",
      "Epoch 336/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0507 - acc: 0.9835 - val_loss: 0.2171 - val_acc: 0.9594\n",
      "Epoch 337/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0513 - acc: 0.9797 - val_loss: 0.2342 - val_acc: 0.9594\n",
      "Epoch 338/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0486 - acc: 0.9784 - val_loss: 0.2344 - val_acc: 0.9594\n",
      "Epoch 339/500\n",
      "787/787 [==============================] - 0s 120us/sample - loss: 0.0559 - acc: 0.9797 - val_loss: 0.2363 - val_acc: 0.9645\n",
      "Epoch 340/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0564 - acc: 0.9784 - val_loss: 0.2276 - val_acc: 0.9543\n",
      "Epoch 341/500\n",
      "787/787 [==============================] - 0s 146us/sample - loss: 0.0455 - acc: 0.9809 - val_loss: 0.2256 - val_acc: 0.9543\n",
      "Epoch 342/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.0650 - acc: 0.9771 - val_loss: 0.2333 - val_acc: 0.9594\n",
      "Epoch 343/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0583 - acc: 0.9835 - val_loss: 0.2343 - val_acc: 0.9492\n",
      "Epoch 344/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.2290 - val_acc: 0.9594\n",
      "Epoch 345/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0539 - acc: 0.9771 - val_loss: 0.2348 - val_acc: 0.9594\n",
      "Epoch 346/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0651 - acc: 0.9746 - val_loss: 0.2067 - val_acc: 0.9594\n",
      "Epoch 347/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0551 - acc: 0.9809 - val_loss: 0.1945 - val_acc: 0.9543\n",
      "Epoch 348/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0365 - acc: 0.9860 - val_loss: 0.2046 - val_acc: 0.9594\n",
      "Epoch 349/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0461 - acc: 0.9759 - val_loss: 0.2242 - val_acc: 0.9594\n",
      "Epoch 350/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0467 - acc: 0.9809 - val_loss: 0.2083 - val_acc: 0.9594\n",
      "Epoch 351/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0383 - acc: 0.9860 - val_loss: 0.2208 - val_acc: 0.9594\n",
      "Epoch 352/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0537 - acc: 0.9797 - val_loss: 0.2303 - val_acc: 0.9594\n",
      "Epoch 353/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0537 - acc: 0.9784 - val_loss: 0.2286 - val_acc: 0.9594\n",
      "Epoch 354/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0559 - acc: 0.9759 - val_loss: 0.2261 - val_acc: 0.9594\n",
      "Epoch 355/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0589 - acc: 0.9797 - val_loss: 0.2206 - val_acc: 0.9594\n",
      "Epoch 356/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0427 - acc: 0.9835 - val_loss: 0.2164 - val_acc: 0.9543\n",
      "Epoch 357/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0468 - acc: 0.9835 - val_loss: 0.2256 - val_acc: 0.9543\n",
      "Epoch 358/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0595 - acc: 0.9746 - val_loss: 0.2334 - val_acc: 0.9645\n",
      "Epoch 359/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0363 - acc: 0.9898 - val_loss: 0.2281 - val_acc: 0.9594\n",
      "Epoch 360/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0533 - acc: 0.9797 - val_loss: 0.2358 - val_acc: 0.9645\n",
      "Epoch 361/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0532 - acc: 0.9771 - val_loss: 0.2341 - val_acc: 0.9645\n",
      "Epoch 362/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0612 - acc: 0.9784 - val_loss: 0.2159 - val_acc: 0.9645\n",
      "Epoch 363/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.2154 - val_acc: 0.9594\n",
      "Epoch 364/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0565 - acc: 0.9848 - val_loss: 0.2322 - val_acc: 0.9645\n",
      "Epoch 365/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0433 - acc: 0.9797 - val_loss: 0.2291 - val_acc: 0.9594\n",
      "Epoch 366/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0374 - acc: 0.9848 - val_loss: 0.2390 - val_acc: 0.9645\n",
      "Epoch 367/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0510 - acc: 0.9848 - val_loss: 0.2218 - val_acc: 0.9594\n",
      "Epoch 368/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.0490 - acc: 0.9848 - val_loss: 0.2378 - val_acc: 0.9594\n",
      "Epoch 369/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0664 - acc: 0.9746 - val_loss: 0.2315 - val_acc: 0.9645\n",
      "Epoch 370/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0410 - acc: 0.9835 - val_loss: 0.2476 - val_acc: 0.9645\n",
      "Epoch 371/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0497 - acc: 0.9809 - val_loss: 0.2463 - val_acc: 0.9645\n",
      "Epoch 372/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0408 - acc: 0.9822 - val_loss: 0.2407 - val_acc: 0.9645\n",
      "Epoch 373/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0499 - acc: 0.9822 - val_loss: 0.2355 - val_acc: 0.9594\n",
      "Epoch 374/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0347 - acc: 0.9886 - val_loss: 0.2354 - val_acc: 0.9594\n",
      "Epoch 375/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0300 - acc: 0.9898 - val_loss: 0.2401 - val_acc: 0.9594\n",
      "Epoch 376/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0345 - acc: 0.9835 - val_loss: 0.2493 - val_acc: 0.9594\n",
      "Epoch 377/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0452 - acc: 0.9797 - val_loss: 0.2416 - val_acc: 0.9594\n",
      "Epoch 378/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0269 - acc: 0.9873 - val_loss: 0.2518 - val_acc: 0.9594\n",
      "Epoch 379/500\n",
      "787/787 [==============================] - 0s 119us/sample - loss: 0.0353 - acc: 0.9873 - val_loss: 0.2448 - val_acc: 0.9594\n",
      "Epoch 380/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0477 - acc: 0.9809 - val_loss: 0.2380 - val_acc: 0.9594\n",
      "Epoch 381/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0430 - acc: 0.9873 - val_loss: 0.2342 - val_acc: 0.9594\n",
      "Epoch 382/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0407 - acc: 0.9848 - val_loss: 0.2364 - val_acc: 0.9543\n",
      "Epoch 383/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0471 - acc: 0.9835 - val_loss: 0.2371 - val_acc: 0.9594\n",
      "Epoch 384/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0398 - acc: 0.9848 - val_loss: 0.2330 - val_acc: 0.9594\n",
      "Epoch 385/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0532 - acc: 0.9797 - val_loss: 0.2290 - val_acc: 0.9594\n",
      "Epoch 386/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0458 - acc: 0.9848 - val_loss: 0.2356 - val_acc: 0.9594\n",
      "Epoch 387/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0460 - acc: 0.9784 - val_loss: 0.2347 - val_acc: 0.9594\n",
      "Epoch 388/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0493 - acc: 0.9809 - val_loss: 0.2430 - val_acc: 0.9594\n",
      "Epoch 389/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0520 - acc: 0.9822 - val_loss: 0.2326 - val_acc: 0.9645\n",
      "Epoch 390/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0424 - acc: 0.9848 - val_loss: 0.2314 - val_acc: 0.9645\n",
      "Epoch 391/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0432 - acc: 0.9809 - val_loss: 0.2382 - val_acc: 0.9645\n",
      "Epoch 392/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0539 - acc: 0.9835 - val_loss: 0.2316 - val_acc: 0.9594\n",
      "Epoch 393/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0377 - acc: 0.9835 - val_loss: 0.2436 - val_acc: 0.9594\n",
      "Epoch 394/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0448 - acc: 0.9835 - val_loss: 0.2368 - val_acc: 0.9594\n",
      "Epoch 395/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0632 - acc: 0.9797 - val_loss: 0.2335 - val_acc: 0.9645\n",
      "Epoch 396/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0508 - acc: 0.9809 - val_loss: 0.2107 - val_acc: 0.9645\n",
      "Epoch 397/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0449 - acc: 0.9848 - val_loss: 0.2174 - val_acc: 0.9594\n",
      "Epoch 398/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0359 - acc: 0.9848 - val_loss: 0.2170 - val_acc: 0.9594\n",
      "Epoch 399/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0371 - acc: 0.9898 - val_loss: 0.2255 - val_acc: 0.9594\n",
      "Epoch 400/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0486 - acc: 0.9848 - val_loss: 0.2256 - val_acc: 0.9543\n",
      "Epoch 401/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0699 - acc: 0.9784 - val_loss: 0.2515 - val_acc: 0.9645\n",
      "Epoch 402/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0537 - acc: 0.9886 - val_loss: 0.2437 - val_acc: 0.9645\n",
      "Epoch 403/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0472 - acc: 0.9784 - val_loss: 0.2335 - val_acc: 0.9645\n",
      "Epoch 404/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0507 - acc: 0.9848 - val_loss: 0.2130 - val_acc: 0.9645\n",
      "Epoch 405/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0516 - acc: 0.9873 - val_loss: 0.2093 - val_acc: 0.9645\n",
      "Epoch 406/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0409 - acc: 0.9860 - val_loss: 0.2018 - val_acc: 0.9594\n",
      "Epoch 407/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0371 - acc: 0.9873 - val_loss: 0.1963 - val_acc: 0.9543\n",
      "Epoch 408/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0400 - acc: 0.9835 - val_loss: 0.2008 - val_acc: 0.9594\n",
      "Epoch 409/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0388 - acc: 0.9848 - val_loss: 0.2004 - val_acc: 0.9645\n",
      "Epoch 410/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0403 - acc: 0.9822 - val_loss: 0.2116 - val_acc: 0.9695\n",
      "Epoch 411/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0375 - acc: 0.9873 - val_loss: 0.2152 - val_acc: 0.9594\n",
      "Epoch 412/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0597 - acc: 0.9835 - val_loss: 0.2187 - val_acc: 0.9645\n",
      "Epoch 413/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0413 - acc: 0.9822 - val_loss: 0.2082 - val_acc: 0.9594\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0450 - acc: 0.9797 - val_loss: 0.2124 - val_acc: 0.9594\n",
      "Epoch 415/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0359 - acc: 0.9809 - val_loss: 0.2178 - val_acc: 0.9594\n",
      "Epoch 416/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0600 - acc: 0.9784 - val_loss: 0.2193 - val_acc: 0.9645\n",
      "Epoch 417/500\n",
      "787/787 [==============================] - 0s 117us/sample - loss: 0.0346 - acc: 0.9848 - val_loss: 0.2223 - val_acc: 0.9645\n",
      "Epoch 418/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0457 - acc: 0.9835 - val_loss: 0.2190 - val_acc: 0.9543\n",
      "Epoch 419/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0677 - acc: 0.9784 - val_loss: 0.2303 - val_acc: 0.9594\n",
      "Epoch 420/500\n",
      "787/787 [==============================] - 0s 136us/sample - loss: 0.0467 - acc: 0.9822 - val_loss: 0.2155 - val_acc: 0.9594\n",
      "Epoch 421/500\n",
      "787/787 [==============================] - 0s 132us/sample - loss: 0.0614 - acc: 0.9860 - val_loss: 0.2170 - val_acc: 0.9594\n",
      "Epoch 422/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0451 - acc: 0.9848 - val_loss: 0.2191 - val_acc: 0.9695\n",
      "Epoch 423/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0344 - acc: 0.9860 - val_loss: 0.2258 - val_acc: 0.9695\n",
      "Epoch 424/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0592 - acc: 0.9746 - val_loss: 0.2172 - val_acc: 0.9645\n",
      "Epoch 425/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0431 - acc: 0.9886 - val_loss: 0.2156 - val_acc: 0.9645\n",
      "Epoch 426/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0475 - acc: 0.9784 - val_loss: 0.2164 - val_acc: 0.9645\n",
      "Epoch 427/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0321 - acc: 0.9898 - val_loss: 0.2213 - val_acc: 0.9645\n",
      "Epoch 428/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0324 - acc: 0.9911 - val_loss: 0.2208 - val_acc: 0.9695\n",
      "Epoch 429/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.0578 - acc: 0.9835 - val_loss: 0.2116 - val_acc: 0.9695\n",
      "Epoch 430/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0530 - acc: 0.9809 - val_loss: 0.2009 - val_acc: 0.9594\n",
      "Epoch 431/500\n",
      "787/787 [==============================] - 0s 131us/sample - loss: 0.0479 - acc: 0.9835 - val_loss: 0.2007 - val_acc: 0.9645\n",
      "Epoch 432/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0531 - acc: 0.9797 - val_loss: 0.2121 - val_acc: 0.9645\n",
      "Epoch 433/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0358 - acc: 0.9886 - val_loss: 0.2141 - val_acc: 0.9645\n",
      "Epoch 434/500\n",
      "787/787 [==============================] - 0s 138us/sample - loss: 0.0507 - acc: 0.9835 - val_loss: 0.2083 - val_acc: 0.9645\n",
      "Epoch 435/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0454 - acc: 0.9809 - val_loss: 0.2098 - val_acc: 0.9645\n",
      "Epoch 436/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0731 - acc: 0.9797 - val_loss: 0.2160 - val_acc: 0.9594\n",
      "Epoch 437/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0427 - acc: 0.9835 - val_loss: 0.2192 - val_acc: 0.9594\n",
      "Epoch 438/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0302 - acc: 0.9873 - val_loss: 0.2287 - val_acc: 0.9543\n",
      "Epoch 439/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0312 - acc: 0.9848 - val_loss: 0.2341 - val_acc: 0.9543\n",
      "Epoch 440/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.0309 - acc: 0.9848 - val_loss: 0.2331 - val_acc: 0.9594\n",
      "Epoch 441/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0431 - acc: 0.9848 - val_loss: 0.2292 - val_acc: 0.9543\n",
      "Epoch 442/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0542 - acc: 0.9797 - val_loss: 0.2261 - val_acc: 0.9543\n",
      "Epoch 443/500\n",
      "787/787 [==============================] - 0s 135us/sample - loss: 0.0367 - acc: 0.9873 - val_loss: 0.2266 - val_acc: 0.9543\n",
      "Epoch 444/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0399 - acc: 0.9898 - val_loss: 0.2342 - val_acc: 0.9594\n",
      "Epoch 445/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0418 - acc: 0.9873 - val_loss: 0.2423 - val_acc: 0.9543\n",
      "Epoch 446/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0330 - acc: 0.9886 - val_loss: 0.2424 - val_acc: 0.9543\n",
      "Epoch 447/500\n",
      "787/787 [==============================] - 0s 132us/sample - loss: 0.0439 - acc: 0.9809 - val_loss: 0.2386 - val_acc: 0.9543\n",
      "Epoch 448/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0513 - acc: 0.9822 - val_loss: 0.2387 - val_acc: 0.9594\n",
      "Epoch 449/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0464 - acc: 0.9860 - val_loss: 0.2422 - val_acc: 0.9594\n",
      "Epoch 450/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0451 - acc: 0.9860 - val_loss: 0.2326 - val_acc: 0.9594\n",
      "Epoch 451/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0336 - acc: 0.9886 - val_loss: 0.2371 - val_acc: 0.9543\n",
      "Epoch 452/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0319 - acc: 0.9911 - val_loss: 0.2322 - val_acc: 0.9543\n",
      "Epoch 453/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0580 - acc: 0.9873 - val_loss: 0.2259 - val_acc: 0.9594\n",
      "Epoch 454/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.0459 - acc: 0.9848 - val_loss: 0.2180 - val_acc: 0.9543\n",
      "Epoch 455/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0486 - acc: 0.9809 - val_loss: 0.2172 - val_acc: 0.9543\n",
      "Epoch 456/500\n",
      "787/787 [==============================] - 0s 137us/sample - loss: 0.0592 - acc: 0.9822 - val_loss: 0.2226 - val_acc: 0.9543\n",
      "Epoch 457/500\n",
      "787/787 [==============================] - 0s 133us/sample - loss: 0.0331 - acc: 0.9898 - val_loss: 0.2282 - val_acc: 0.9543\n",
      "Epoch 458/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0528 - acc: 0.9797 - val_loss: 0.2252 - val_acc: 0.9543\n",
      "Epoch 459/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0400 - acc: 0.9835 - val_loss: 0.2278 - val_acc: 0.9543\n",
      "Epoch 460/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0476 - acc: 0.9822 - val_loss: 0.2408 - val_acc: 0.9594\n",
      "Epoch 461/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0300 - acc: 0.9898 - val_loss: 0.2371 - val_acc: 0.9594\n",
      "Epoch 462/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0433 - acc: 0.9848 - val_loss: 0.2367 - val_acc: 0.9594\n",
      "Epoch 463/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0400 - acc: 0.9848 - val_loss: 0.2460 - val_acc: 0.9594\n",
      "Epoch 464/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0403 - acc: 0.9835 - val_loss: 0.2196 - val_acc: 0.9594\n",
      "Epoch 465/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0358 - acc: 0.9873 - val_loss: 0.2346 - val_acc: 0.9594\n",
      "Epoch 466/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0338 - acc: 0.9822 - val_loss: 0.2381 - val_acc: 0.9594\n",
      "Epoch 467/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0363 - acc: 0.9860 - val_loss: 0.2358 - val_acc: 0.9543\n",
      "Epoch 468/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0376 - acc: 0.9860 - val_loss: 0.2353 - val_acc: 0.9594\n",
      "Epoch 469/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0465 - acc: 0.9886 - val_loss: 0.2285 - val_acc: 0.9543\n",
      "Epoch 470/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0566 - acc: 0.9809 - val_loss: 0.2289 - val_acc: 0.9543\n",
      "Epoch 471/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0278 - acc: 0.9924 - val_loss: 0.2261 - val_acc: 0.9543\n",
      "Epoch 472/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0443 - acc: 0.9822 - val_loss: 0.2375 - val_acc: 0.9594\n",
      "Epoch 473/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0384 - acc: 0.9822 - val_loss: 0.2308 - val_acc: 0.9594\n",
      "Epoch 474/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0364 - acc: 0.9886 - val_loss: 0.2219 - val_acc: 0.9594\n",
      "Epoch 475/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0524 - acc: 0.9771 - val_loss: 0.2328 - val_acc: 0.9594\n",
      "Epoch 476/500\n",
      "787/787 [==============================] - 0s 134us/sample - loss: 0.0501 - acc: 0.9835 - val_loss: 0.2339 - val_acc: 0.9594\n",
      "Epoch 477/500\n",
      "787/787 [==============================] - 0s 139us/sample - loss: 0.0337 - acc: 0.9886 - val_loss: 0.2349 - val_acc: 0.9594\n",
      "Epoch 478/500\n",
      "787/787 [==============================] - 0s 135us/sample - loss: 0.0368 - acc: 0.9771 - val_loss: 0.2359 - val_acc: 0.9645\n",
      "Epoch 479/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0289 - acc: 0.9873 - val_loss: 0.2378 - val_acc: 0.9543\n",
      "Epoch 480/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0324 - acc: 0.9860 - val_loss: 0.2431 - val_acc: 0.9543\n",
      "Epoch 481/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0415 - acc: 0.9873 - val_loss: 0.2398 - val_acc: 0.9594\n",
      "Epoch 482/500\n",
      "787/787 [==============================] - 0s 130us/sample - loss: 0.0390 - acc: 0.9873 - val_loss: 0.2547 - val_acc: 0.9594\n",
      "Epoch 483/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0196 - acc: 0.9936 - val_loss: 0.2596 - val_acc: 0.9645\n",
      "Epoch 484/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0607 - acc: 0.9809 - val_loss: 0.2323 - val_acc: 0.9543\n",
      "Epoch 485/500\n",
      "787/787 [==============================] - 0s 122us/sample - loss: 0.0338 - acc: 0.9898 - val_loss: 0.2228 - val_acc: 0.9543\n",
      "Epoch 486/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0370 - acc: 0.9873 - val_loss: 0.2124 - val_acc: 0.9594\n",
      "Epoch 487/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0300 - acc: 0.9860 - val_loss: 0.2177 - val_acc: 0.9543\n",
      "Epoch 488/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0307 - acc: 0.9848 - val_loss: 0.2358 - val_acc: 0.9594\n",
      "Epoch 489/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0291 - acc: 0.9860 - val_loss: 0.2482 - val_acc: 0.9594\n",
      "Epoch 490/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0378 - acc: 0.9873 - val_loss: 0.2543 - val_acc: 0.9594\n",
      "Epoch 491/500\n",
      "787/787 [==============================] - 0s 126us/sample - loss: 0.0425 - acc: 0.9759 - val_loss: 0.2602 - val_acc: 0.9594\n",
      "Epoch 492/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0464 - acc: 0.9873 - val_loss: 0.2433 - val_acc: 0.9594\n",
      "Epoch 493/500\n",
      "787/787 [==============================] - 0s 128us/sample - loss: 0.0338 - acc: 0.9835 - val_loss: 0.2374 - val_acc: 0.9594\n",
      "Epoch 494/500\n",
      "787/787 [==============================] - 0s 129us/sample - loss: 0.0375 - acc: 0.9860 - val_loss: 0.2380 - val_acc: 0.9594\n",
      "Epoch 495/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0439 - acc: 0.9822 - val_loss: 0.2419 - val_acc: 0.9594\n",
      "Epoch 496/500\n",
      "787/787 [==============================] - 0s 127us/sample - loss: 0.0394 - acc: 0.9873 - val_loss: 0.2231 - val_acc: 0.9594\n",
      "Epoch 497/500\n",
      "787/787 [==============================] - 0s 124us/sample - loss: 0.0249 - acc: 0.9886 - val_loss: 0.2294 - val_acc: 0.9594\n",
      "Epoch 498/500\n",
      "787/787 [==============================] - 0s 121us/sample - loss: 0.0372 - acc: 0.9886 - val_loss: 0.2404 - val_acc: 0.9594\n",
      "Epoch 499/500\n",
      "787/787 [==============================] - 0s 125us/sample - loss: 0.0431 - acc: 0.9822 - val_loss: 0.2403 - val_acc: 0.9543\n",
      "Epoch 500/500\n",
      "787/787 [==============================] - 0s 123us/sample - loss: 0.0486 - acc: 0.9784 - val_loss: 0.2438 - val_acc: 0.9594\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(X_train, y_train, epochs=500, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1,epochs+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_range, result.history['acc'])\n",
    "plt.plot(epoch_range, result.history['val_acc'])\n",
    "plt.title('Model\\'s Accuracy')\n",
    "plt.xlabel('No. of Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
